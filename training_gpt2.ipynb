{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:10:16.362105Z",
     "start_time": "2023-12-03T21:10:16.348435Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "file_path = 'qa_dataset.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:10:16.374627Z",
     "start_time": "2023-12-03T21:10:16.356665Z"
    }
   },
   "id": "336e3f59095cf075"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:10:16.969735Z",
     "start_time": "2023-12-03T21:10:16.366448Z"
    }
   },
   "id": "29c40423370cfb0c"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Add a padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Assuming binary classification (change as needed)\n",
    "labels = torch.tensor([1 if x['answer_length'] > 100 else 0 for x in data])\n",
    "\n",
    "# Tokenize and truncate long sequences\n",
    "tokenized_data = tokenizer(\n",
    "    [x['question'] + \" [SEP] \" + x['answer'] for x in data],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512  # Adjust the max_length based on your model's maximum sequence length\n",
    ")\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(tokenized_data['input_ids'], tokenized_data['attention_mask'], labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:10:17.384624Z",
     "start_time": "2023-12-03T21:10:16.969498Z"
    }
   },
   "id": "d804d2853e26b325"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:11:16.965992Z",
     "start_time": "2023-12-03T21:11:14.955657Z"
    }
   },
   "id": "9b2a94ce5138fac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "\n",
    "# Data loader\n",
    "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "# Classification head for binary classification\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Loss function for binary classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished. Loss: {total_loss / len(dataloader)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-03T21:12:21.477673Z"
    }
   },
   "id": "5a16023ef51bad72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    predictions.extend(batch_preds)\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T21:10:19.359457Z"
    }
   },
   "id": "2598b529f5b9ffaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:10:19.375999Z",
     "start_time": "2023-12-03T21:10:19.361662Z"
    }
   },
   "id": "7fa3aedd057a821c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
