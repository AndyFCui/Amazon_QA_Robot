{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e3f59095cf075",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'qa_dataset.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c40423370cfb0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30901295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original \n",
    "# Prepare data \n",
    "\n",
    "# Tokenizing the data\n",
    "# inputs = tokenizer([x['question'] + \" [SEP] \" + x['answer'] for x in data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Assuming binary classification (change as needed)\n",
    "# labels = torch.tensor([1 if x['answer_length'] > 100 else 0 for x in data])\n",
    "\n",
    "# Create a dataset\n",
    "# dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "# Tokenizing the data\n",
    "max_length = max(len(tokenizer.encode(x['question'] + \" [SEP] \" + x['answer'])) for x in data)\n",
    "tokenized_data = [tokenizer(x['question'] + \" [SEP] \" + x['answer'], \n",
    "                            padding='max_length',  \n",
    "                            max_length=max_length,  \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\") for x in data]\n",
    "\n",
    "# input\n",
    "input_ids = torch.cat([item['input_ids'] for item in tokenized_data], dim=0)\n",
    "attention_masks = torch.cat([item['attention_mask'] for item in tokenized_data], dim=0)\n",
    "labels = torch.tensor([1 if x['answer_length'] > 100 else 0 for x in data])\n",
    "\n",
    "# dataset seprate\n",
    "input_ids_train, input_ids_test, attention_masks_train, attention_masks_test, labels_train, labels_test = train_test_split(\n",
    "    input_ids, attention_masks, labels, train_size=0.7, random_state=42)\n",
    "\n",
    "# create dataset\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a94ce5138fac5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU is work\n",
    "# Train the model using a suitable optimizer and loss function.\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc82c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b226d5e9330d47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config 1-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16023ef51bad72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size_list = [4, 8]\n",
    "epochs_list = [25, 30]\n",
    "learning_rate_list = [1e-5, 1e-4]\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    for epochs in epochs_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "            \n",
    "            print(f'Parameters: \\n    Batch size: {batch_size}\\n    Epochs: {epochs}\\n    Learning rate:{learning_rate}\\n')\n",
    "            # Prepare for epoch_losses\n",
    "            epoch_losses = []\n",
    "            \n",
    "            # Create dataloader\n",
    "            train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "            test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "            \n",
    "            # Optimizer\n",
    "            optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "            \n",
    "                for batch in progress_bar:\n",
    "                    # b_input_ids, b_input_mask, b_labels = batch\n",
    "                    b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "                    model.zero_grad()\n",
    "            \n",
    "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                    loss = outputs.loss\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "                    # Update the progress bar\n",
    "                    progress_bar.set_postfix({'loss': total_loss/len(train_dataloader)})\n",
    "                    \n",
    "                # Calculate and store the average loss for this epoch\n",
    "                avg_loss = total_loss / len(train_dataloader)\n",
    "                epoch_losses.append(avg_loss)\n",
    "                \n",
    "                # Save the model after each epoch\n",
    "                # model_save_file = os.path.join(model_save_path, f'bert_model_epoch_{epoch+1}.pt')\n",
    "                # torch.save(model.state_dict(), model_save_file)\n",
    "            \n",
    "                # Closing the progress bar and printing the epoch loss\n",
    "                progress_bar.close()\n",
    "                # print(f\"Epoch {epoch+1} finished. Loss: {total_loss/len(train_dataloader)}\")\n",
    "            print(f'Loss on training: {epoch_losses}')\n",
    "\n",
    "            # Switch to evaluation mode\n",
    "            model.eval()\n",
    "            predictions, true_labels = [], []\n",
    "            # Add tqdm progress bar\n",
    "            for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "                batch_preds = np.argmax(logits, axis=1)\n",
    "                predictions.extend(batch_preds)\n",
    "                true_labels.extend(label_ids)\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            accuracy = accuracy_score(true_labels, predictions)\n",
    "            print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "            # Plotting the training loss\n",
    "            plt.plot(epoch_losses, label='Training Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss Over Epochs')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32cb9c7a9fea54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1881fd2537daf3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 2\n",
    "# Prepare for epoch_losses\n",
    "epoch_losses = []\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "model.train()\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix({'loss': total_loss/len(train_dataloader)})\n",
    "        \n",
    "    # Calculate and store the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    # model_save_file = os.path.join(model_save_path, f'bert_model_epoch_{epoch+1}.pt')\n",
    "    # torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "    # Closing the progress bar and printing the epoch loss\n",
    "    progress_bar.close()\n",
    "    print(f\"Epoch {epoch+1} finished. Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "# Add tqdm progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    predictions.extend(batch_preds)\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c15225f1d5a1e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1130d4b692390",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 8\n",
    "# Prepare for epoch_losses\n",
    "epoch_losses = []\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix({'loss': total_loss/len(train_dataloader)})\n",
    "        \n",
    "    # Calculate and store the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    # model_save_file = os.path.join(model_save_path, f'bert_model_epoch_{epoch+1}.pt')\n",
    "    # torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "    # Closing the progress bar and printing the epoch loss\n",
    "    progress_bar.close()\n",
    "    print(f\"Epoch {epoch+1} finished. Loss: {total_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'models'  \n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "final_model_save_file = os.path.join(model_save_path, 'bert_final_model_1.pt')\n",
    "torch.save(model.state_dict(), final_model_save_file)\n",
    "\n",
    "# print paramter\n",
    "print(\"Training Parameters:\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Learning Rate: {optimizer.defaults['lr']}\")\n",
    "print(f\"Beta1: {optimizer.defaults['betas'][0]}\")\n",
    "print(f\"Beta2: {optimizer.defaults['betas'][1]}\")\n",
    "\n",
    "print(f\"Training completed. Final model saved to {final_model_save_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training loss\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6d5d9",
   "metadata": {},
   "source": [
    "### Evulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598b529f5b9ffaa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Add tqdm progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    predictions.extend(batch_preds)\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de4e7b",
   "metadata": {},
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e96fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize performance metric list\n",
    "fold_performance = []\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "epochs = 25\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Start the KFold cross-validation\n",
    "for fold, (train_ids, test_ids) in enumerate(kf.split(dataset)):\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_subset, sampler=RandomSampler(train_subset), batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_subset, sampler=SequentialSampler(test_subset), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize the BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop for the current fold\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{epochs}, Fold {fold+1}/10\")\n",
    "        \n",
    "        for step, batch in enumerate(train_progress_bar):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_progress_bar.set_postfix(loss=total_loss/(step+1))\n",
    "        \n",
    "        train_progress_bar.close()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    eval_progress_bar = tqdm(test_dataloader, desc=f\"Validation, Fold {fold+1}/10\")\n",
    "\n",
    "    for batch in eval_progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "    \n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        batch_accuracy = np.mean(preds == label_ids)\n",
    "        total_eval_accuracy += batch_accuracy\n",
    "        \n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(f\"Validation accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    fold_performance.append({\n",
    "        'fold': fold,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': avg_val_accuracy\n",
    "    })\n",
    "\n",
    "    eval_progress_bar.close()\n",
    "\n",
    "# Calculate and print the average performance across all folds\n",
    "average_performance = {\n",
    "    'avg_train_loss': np.mean([x['train_loss'] for x in fold_performance]),\n",
    "    'avg_val_loss': np.mean([x['val_loss'] for x in fold_performance]),\n",
    "    'avg_val_accuracy': np.mean([x['val_accuracy'] for x in fold_performance])\n",
    "}\n",
    "print(f\"Average performance across all folds: {average_performance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
